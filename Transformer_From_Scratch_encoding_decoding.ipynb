{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPCKlT52YnNVjAf0tzJyRk/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pinkfloyed/Transformer_Scratch_Code/blob/main/Transformer_From_Scratch_encoding_decoding.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, torch.nn as nn, torch.optim as optim, torch.utils.data as data, math, copy"
      ],
      "metadata": {
        "id": "wIJUvyGIzoKg"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy.ma.core import indices\n",
        "from torch.ao.nn.quantized import ReLU6\n",
        "from torch.nn import ReLU\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "XnDTrgJHzoG7"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dimension_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.dimension_head = d_model // num_heads\n",
        "\n",
        "        self.weight_query = nn.Linear(d_model, d_model)\n",
        "        self.weight_key = nn.Linear(d_model, d_model)\n",
        "        self.weight_value = nn.Linear(d_model, d_model)\n",
        "        self.weight_output = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, query, key, value, mask = None):\n",
        "        attention_scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.dimension_head)\n",
        "        if mask is not None:\n",
        "            attention_scores = attention_scores.masked_fill(mask == 0, -1e9)\n",
        "        attention_probs = torch.softmax(attention_scores, dim = -1)\n",
        "        output = torch.matmul(attention_probs, value)\n",
        "        return output\n",
        "\n",
        "    def split_heads(self, x):\n",
        "        batch_size, seq_length, d_model = x.size()\n",
        "        return x.view(batch_size, seq_length, self.num_heads, self.dimension_head).transpose(1,2)\n",
        "\n",
        "    def combine_heads(self, x):\n",
        "        batch_size, _, seq_length, d_k = x.size()\n",
        "        return x.transpose(1,2).contiguous().view(batch_size, seq_length, self.dimension_model)\n",
        "\n",
        "    def forward(self, query, key, value, mask = None):\n",
        "        query = self.split_heads(self.weight_query(query))\n",
        "        key = self.split_heads(self.weight_key(key))\n",
        "        value = self.split_heads(self.weight_value(value))\n",
        "\n",
        "        attention_output = self.scaled_dot_product_attention(query, key, value, mask)\n",
        "        output = self.weight_output(self.combine_heads(attention_output))\n",
        "        return output"
      ],
      "metadata": {
        "id": "eT-w2-0vzoDp"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(d_model, d_ff)\n",
        "        self.fc2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc2(self.relu(self.fc1(x)))"
      ],
      "metadata": {
        "id": "1vJ1N-ZjzoAX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_seq_length):\n",
        "       super().__init__()\n",
        "\n",
        "       pe = torch.zeros(max_seq_length, d_model)\n",
        "       position = torch.arange(0, max_seq_length, dtype = torch.float).unsqueeze(1)\n",
        "       div_term = torch.exp(torch.arange(0, d_model, 2).float()* -(math.log(10000.0)/d_model))\n",
        "\n",
        "       pe[:, ::2] = torch.sin(position*div_term)\n",
        "       pe[:, 1::2] = torch.cos(position * div_term)\n",
        "\n",
        "       self.register_buffer('pe', pe.unsqueeze(0))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1)]"
      ],
      "metadata": {
        "id": "C5i8szwk0DGa"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask):\n",
        "        attn_output = self.self_attn(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm2(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "7b9KhNUL0DDO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
        "        super().__init__()\n",
        "        self.self_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.cross_attn = MultiHeadAttention(d_model, num_heads)\n",
        "        self.feed_forward = PositionWiseFeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_out, src_mask, tgt_mask):\n",
        "        attn_output = self.self_attn(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        attn_output = self.cross_attn(x, enc_out, enc_out, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn_output))\n",
        "        ff_output = self.feed_forward(x)\n",
        "        x = self.norm3(x + self.dropout(ff_output))\n",
        "        return x"
      ],
      "metadata": {
        "id": "0t6F25XZ0Nuc"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout):\n",
        "        super().__init__()\n",
        "        self.encoder_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.decoder_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.positional_encoding = PositionalEncoding(d_model, max_seq_length)\n",
        "\n",
        "        self.encoder_layers = nn.ModuleList(\n",
        "            [EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList(\n",
        "            [DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "\n",
        "        self.fc = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != 0).unsqueeze(1).unsqueeze(2)\n",
        "        seq_length = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(tgt)))\n",
        "\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "        output = self.fc(dec_output)\n",
        "\n",
        "        return output\n",
        "\n",
        "\n",
        "    def generate(self, src, start_token, max_length, temperature=1.0, top_k=None):\n",
        "        self.eval()\n",
        "        src_mask = (src != 0).unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        src_embedded = self.dropout(self.positional_encoding(self.encoder_embedding(src)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "\n",
        "        generated = torch.tensor([[start_token]], dtype=torch.long)\n",
        "\n",
        "        for _ in range(max_length - 1):\n",
        "            tgt_mask = (generated != 0).unsqueeze(1).unsqueeze(2)\n",
        "            seq_length = generated.size(1)\n",
        "            nopeak_mask = (1 - torch.triu(torch.ones(seq_length, seq_length), diagonal=1)).bool()\n",
        "            tgt_mask = tgt_mask & nopeak_mask\n",
        "\n",
        "            tgt_embedded = self.dropout(self.positional_encoding(self.decoder_embedding(generated)))\n",
        "            dec_output = tgt_embedded\n",
        "            for dec_layer in self.decoder_layers:\n",
        "                dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "\n",
        "            logits = self.fc(dec_output[:, -1, :]) / temperature\n",
        "            if top_k is not None:\n",
        "                top_k = min(top_k, logits.size(-1))\n",
        "                values, indices = torch.topk(logits, top_k)\n",
        "                logits = torch.full_like(logits, float('-inf')).scatter(-1, indices, values)\n",
        "\n",
        "            probs = nn.functional.softmax(logits, dim=-1)\n",
        "            next_token = torch.multinomial(probs, num_samples=1)\n",
        "\n",
        "            generated = torch.cat([generated, next_token], dim=1)\n",
        "\n",
        "            if len(generated[0]) == max_length:\n",
        "                break\n",
        "        return generated"
      ],
      "metadata": {
        "id": "5Mx5oSq20NrG"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/input.txt', 'r', encoding='utf-8') as file:\n",
        "    text = file.read()"
      ],
      "metadata": {
        "id": "HZZKi9ec0U43"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chars = sorted(list(set(text)))\n",
        "v_size = len(chars)"
      ],
      "metadata": {
        "id": "pImvlVZS0XJE"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "src_vocab_size = v_size\n",
        "tgt_vocab_size = v_size\n",
        "d_model = 512\n",
        "num_heads = 8\n",
        "num_layers = 6\n",
        "d_ff = 2048\n",
        "max_seq_length =100\n",
        "dropout = 0.1"
      ],
      "metadata": {
        "id": "Bd-oyz-L0XFu"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "string_to_int = {c: i for i, c in enumerate(chars)}\n",
        "int_to_string = {i: c for i, c in enumerate(chars)}"
      ],
      "metadata": {
        "id": "Enbxankj0bGY"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encode = lambda s: [string_to_int[c] for c in s]\n",
        "decode = lambda i: ''.join(int_to_string[c.item()] for c in i)"
      ],
      "metadata": {
        "id": "1F0-NAzP0dOq"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transformer = Transformer(src_vocab_size, tgt_vocab_size, d_model, num_heads, num_layers, d_ff, max_seq_length, dropout)\n",
        "context_with_given_input = torch.tensor([encode('hello')], dtype = torch.long)\n",
        "src = torch.tensor([[1]])\n",
        "generated = transformer.generate(\n",
        "    src = context_with_given_input,\n",
        "    start_token = 2,\n",
        "    max_length = max_seq_length,\n",
        "    temperature = 0.7,\n",
        "    top_k = None\n",
        ")\n",
        "print(decode(generated[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3gt74Rh0dE7",
        "outputId": "fedecb57-054f-405d-ed2d-b44075e2972c"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!PÑe,VuZQ:;HimU[ï&kTI”ÑYn#rQWe\n",
            "y%fïI’ éq“72'aN&'’E—L5Y3G,]ap‘/ñ!uÑKé8N‘q%_mO!Ñu“huYÑGV1QA‘iXO,VuWPGO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "src_data = torch.randint(1, src_vocab_size, (64, max_seq_length))\n",
        "tgt_data = torch.randint(1, tgt_vocab_size, (64, max_seq_length))"
      ],
      "metadata": {
        "id": "cOEso4op0lgn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
        "optimizer = optim.Adam(transformer.parameters(), lr=0.0065, betas=(0.9, 0.98), eps=1e-9)\n",
        "transformer.train()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ybExIk00nfG",
        "outputId": "4481a4d4-5f4b-40a8-aad1-916c5dc52b5b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Transformer(\n",
              "  (encoder_embedding): Embedding(95, 512)\n",
              "  (decoder_embedding): Embedding(95, 512)\n",
              "  (positional_encoding): PositionalEncoding()\n",
              "  (encoder_layers): ModuleList(\n",
              "    (0-5): 6 x EncoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (weight_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_output): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (decoder_layers): ModuleList(\n",
              "    (0-5): 6 x DecoderLayer(\n",
              "      (self_attn): MultiHeadAttention(\n",
              "        (weight_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_output): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (cross_attn): MultiHeadAttention(\n",
              "        (weight_query): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_key): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_value): Linear(in_features=512, out_features=512, bias=True)\n",
              "        (weight_output): Linear(in_features=512, out_features=512, bias=True)\n",
              "      )\n",
              "      (feed_forward): PositionWiseFeedForward(\n",
              "        (fc1): Linear(in_features=512, out_features=2048, bias=True)\n",
              "        (fc2): Linear(in_features=2048, out_features=512, bias=True)\n",
              "        (relu): ReLU()\n",
              "      )\n",
              "      (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "  )\n",
              "  (fc): Linear(in_features=512, out_features=95, bias=True)\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "0fEQbhdmpY5U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ce8dd7de-f85a-4aa3-9e35-dc0ebbe7b002"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch : 1, Loss : 4.7122063636779785\n",
            "Epoch : 2, Loss : 5.265438079833984\n",
            "Epoch : 3, Loss : 5.3804545402526855\n",
            "Epoch : 4, Loss : 5.289546489715576\n",
            "Epoch : 5, Loss : 5.362061023712158\n",
            "tensor([[ 2, 64, 69,  2,  7, 92,  4, 52, 92, 92, 40, 69,  4, 15, 37, 79, 19, 90,\n",
            "         37, 44, 80,  4, 42, 70, 52,  4,  4, 90, 18, 92,  4, 71, 38, 34,  1, 90,\n",
            "         79, 87, 69, 42,  4, 87, 22,  4, 42, 31,  4, 89, 71, 92, 62, 90, 34, 86,\n",
            "         92, 38,  4, 19, 71, 29, 33, 92, 27, 70,  4, 53, 88, 21, 73, 52, 70, 87,\n",
            "         66, 34, 44, 25, 90, 42, 34, 40, 89, 88, 37, 92, 79,  7, 10,  4, 31,  7,\n",
            "         43,  7,  3,  4, 80, 92, 38, 80,  4, 92]])\n"
          ]
        }
      ],
      "source": [
        "for epoch in range(5):\n",
        "    optimizer.zero_grad()\n",
        "    output = transformer(src_data, tgt_data[:, :-1])\n",
        "    loss = criterion(output.contiguous().view(-1, tgt_vocab_size), tgt_data[:,1:].contiguous().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    print(f\"Epoch : {epoch+1}, Loss : {loss.item()}\")\n",
        "\n",
        "generated = transformer.generate(\n",
        "    src=src,\n",
        "    start_token =2,\n",
        "    max_length= max_seq_length,\n",
        "    temperature = 0.7,\n",
        "    top_k = None\n",
        ")\n",
        "print(generated)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "decoded_output = decode(generated[0])\n",
        "print(decoded_output)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ZeN3Ib-rWZy",
        "outputId": "c31cd951-b5ec-43a1-9c6c-c9f726ff3850"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!gl!&”#X””Ll#/Iv3’IPw#NmX##’2”#nJF ’vñlN#ñ6#NC#‘n”e’Fï”J#3nAE”;m#Y—5pXmñiFP9’NFL‘—I”v&)#C&O&\"#w”Jw#”\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_text(prompt, method='nucleus', max_length=100, temperature=0.7, top_k=50, top_p=0.9, num_beams=3):\n",
        "    generator = pipeline(\"text-generation\", model=\"gpt2\")\n",
        "\n",
        "    if method == 'beam':\n",
        "        output = generator(prompt, max_length=max_length, num_return_sequences=1, num_beams=num_beams, early_stopping=True)\n",
        "    elif method == 'top_k':\n",
        "        output = generator(prompt, max_length=max_length, do_sample=True, top_k=top_k, temperature=temperature)\n",
        "    elif method == 'nucleus':\n",
        "        output = generator(prompt, max_length=max_length, do_sample=True, top_p=top_p, temperature=temperature)\n",
        "    else:\n",
        "        raise ValueError(\"Invalid decoding method. Choose from 'beam', 'top_k', or 'nucleus'.\")\n",
        "\n",
        "    return output[0]['generated_text']\n"
      ],
      "metadata": {
        "id": "0mQ2M44cCBuU"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    prompt = \"Once upon a time\"\n",
        "    print(\"Beam Search:\", generate_text(prompt, method='beam'))\n",
        "    print(\"Top-K Sampling:\", generate_text(prompt, method='top_k'))\n",
        "    print(\"Nucleus Sampling:\", generate_text(prompt, method='nucleus'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rBHdhDZ_00Cy",
        "outputId": "0a76355d-ccb3-4cd7-ecbe-46277d98b78d"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Beam Search: Once upon a time, there was a man in a black suit who wanted to kill everyone on Earth. He was a man in a black suit who wanted to kill everyone on Earth.\n",
            "\n",
            "This man was the man who was responsible for the destruction of Earth. He was the man who was responsible for the destruction of Earth.\n",
            "\n",
            "The man was the man who was responsible for the destruction of Earth.\n",
            "\n",
            "The man was the man who was responsible for the destruction of Earth.\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top-K Sampling: Once upon a time, we were talking about the \"big three\" - the NFL and the NFLPA. The NFL, which is a small league with a relatively small population, is a small league, with only 1,000 people in the NBA, 2,500 in the NFL and 3,100 in the NBA. The NFL is a small league with a relatively small population, with only 1,000 people in the NBA, 2,500 in the NFL and 3,100 in the NBA\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nucleus Sampling: Once upon a time, we had been given the opportunity to make a decision about the future of the world. But we didn't.\n",
            "\n",
            "Today, the world is still divided. We are not ready for a democratic society, and we are not ready to create a new one. We are not ready to see the future of our children, our grandchildren, our children's children. We are not ready to see a future of a peaceful world. We are not ready to see the future of our\n"
          ]
        }
      ]
    }
  ]
}